{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c55168",
   "metadata": {},
   "source": [
    "Ensuring the data is clean and extracting relevant atrributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#done using pyspark and spark functions\n",
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef4d91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/05 13:37:34 WARN Utils: Your hostname, Cassie-Laptop, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/09/05 13:37:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/05 13:37:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#starting spark session using configuration reccomendations from tutorials\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Data Preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"America/New_York\")\n",
    "    .config('spark.driver.memory', '4g')\n",
    "    .config('spark.executor.memory', '2g')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88bec16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48559814 19\n",
      "260628583 24\n"
     ]
    }
   ],
   "source": [
    "#load TLC datasets\n",
    "yellowDF = spark.read.parquet(\"../datasets/yellow_tlc_data/\")\n",
    "hvfhvDF  = spark.read.parquet(\"../datasets/hvfhv_data/\")\n",
    "\n",
    "#confirm shape\n",
    "print(yellowDF.count(), len(yellowDF.columns))\n",
    "print(hvfhvDF.count(), len(hvfhvDF.columns))\n",
    "\n",
    "#truncate pickup time to only hour of pickup\n",
    "yellowDF = yellowDF.withColumn(\"pickupHour\", F.date_trunc(\"hour\", \n",
    "F.col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "hvfhvDF = hvfhvDF.withColumn(\"pickupHour\", F.date_trunc(\"hour\",\n",
    "F.col(\"pickup_datetime\")))\n",
    "\n",
    "\n",
    "#group by pickup hour\n",
    "yellowCount =(yellowDF.groupBy(\"pickupHour\").agg(F.count(\"*\")\n",
    ".alias(\"yellowCount\")))\n",
    "\n",
    "hvfhvCount = (hvfhvDF.groupBy(\"pickupHour\")\n",
    ".agg(F.count(\"*\").alias(\"hvfhvCount\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4110b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9528 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:========================================================(51 + 0) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9528 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#create final input dataframe\n",
    "\n",
    "#combine counts into one dataframe\n",
    "FinalData = (yellowCount.join(hvfhvCount, on=\"pickupHour\", how=\"outer\").fillna(0))\n",
    "\n",
    "#add ratio\n",
    "FinalData = FinalData.withColumn(\"ratio\", F.col(\"yellowCount\") /\n",
    "(F.col(\"hvfhvCount\") + F.lit(1)))\n",
    "\n",
    "\n",
    "#combine with weather data\n",
    "\n",
    "#loading weather data\n",
    "weatherDF = spark.read.parquet(\"../datasets/weather_data/\")\n",
    "\n",
    "#confirm shape\n",
    "print(weatherDF.count(), len(weatherDF.columns))\n",
    "\n",
    "#rename for joining\n",
    "weatherDF = weatherDF.withColumnRenamed(\"time\", \"pickupHour\")\n",
    "\n",
    "#join\n",
    "FinalData = (FinalData.join(weatherDF, on=\"pickupHour\", how=\"inner\").orderBy(\"pickupHour\"))\n",
    "\n",
    "#confirm shape\n",
    "print(FinalData.count(), len(FinalData.columns))\n",
    "\n",
    "#save the final dataframe- code commented out due to additional preprocessing below\n",
    "\n",
    "#output_path = \"../datasets/processed.parquet\"\n",
    "#FinalData.write.mode(\"overwrite\").parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40227cf1",
   "metadata": {},
   "source": [
    "Additional preprocessing added after initial visualisations, for modelling and for visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c8a62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(pickupHour=datetime.datetime(2024, 3, 1, 16, 0), yellowCount=3249, hvfhvCount=23182, ratio=0.14014579648880646, rain=0.0, temp=-6.8, day=6, weekend=0, rushhour=0, hour=0, raining=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add day of week\n",
    "FinalData = FinalData.withColumn(\n",
    "    \"day\",\n",
    "    F.dayofweek(\"pickupHour\"))\n",
    "\n",
    "#seperate weekend and weekday\n",
    "FinalData = FinalData.withColumn(\n",
    "    \"weekend\",\n",
    "    F.when((F.col(\"day\") == 1) | (F.col(\"day\") == 7), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "#add boolean if rushour or not\n",
    "FinalData = FinalData.withColumn(\n",
    "    \"rushhour\",\n",
    "    F.when(\n",
    "        (F.col(\"weekend\") == 0) & \n",
    "        (F.hour(\"pickupHour\").isin([7,8,9,16,17,18])),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "#add plain hour column\n",
    "FinalData = FinalData.withColumn(\n",
    "    \"hour\",\n",
    "    F.hour(\"pickupHour\")\n",
    ")\n",
    "\n",
    "#add rain threshold boolean\n",
    "FinalData = FinalData.withColumn(\n",
    "    \"raining\", \n",
    "    (F.col(\"rain\") > 0.1).cast(\"int\")   # threshold 0.1mm/hr, adjust if needed\n",
    ")\n",
    "\n",
    "#save final dataframe\n",
    "output_path = \"../datasets/processed.parquet\"\n",
    "FinalData.write.mode(\"overwrite\").parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a654b9",
   "metadata": {},
   "source": [
    "Aditional preprocessing, checking for null values and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061cae1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:======================================================> (50 + 1) / 51]\r"
     ]
    }
   ],
   "source": [
    "#checking no null hours\n",
    "FinalData.filter(F.col(\"rain\").isNull()).count()\n",
    "FinalData.filter(F.col(\"pickupHour\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62046936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: 0 outliers (0.00% of data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellowCount: 0 outliers (0.00% of data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 148:=====================================================> (50 + 1) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvfhvCount: 6 outliers (0.06% of data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#checking for outliers\n",
    "\n",
    "for col in [\"ratio\", \"yellowCount\", \"hvfhvCount\"]:\n",
    "    q1, q3 = FinalData.approxQuantile(col, [0.25, 0.75], 0.01)\n",
    "    IQR = q3 - q1\n",
    "    lower = q1 - 1.5 * IQR\n",
    "    upper = q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = FinalData.filter((F.col(col) < lower) | (F.col(col) > upper)).count()\n",
    "    \n",
    "    print(col, outliers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
